# -*- coding: utf-8 -*-
"""optuna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/TasnimAhmedEee/House-Price-Prediction/blob/master/optuna.ipynb

Example of Implementing Optuna for optimizing Neural Network
"""

#importing libraries

#!pip install optuna
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data
import numpy as np
from torchvision import datasets
from torchvision import transforms

#variables
DEVICE = torch.device("cuda")
BATCHSIZE = 128
CLASSES = 10
DIR = os.getcwd()
EPOCHS = 10
LOG_INTERVAL = 10
N_TRAIN_EXAMPLES = np.multiply(BATCHSIZE , 30)
N_VALID_EXAMPLES = np.multiply(BATCHSIZE , 10)

#Defining neural network
#Variables under "trial" object will be optimized by optuna

def define_model(trial):
    # Letting optuna to optimize the number of layers, hidden units and dropout ratio in each layer.
    n_layers = trial.suggest_int("n_layers", 1, 3)
    layers = []

    in_features = np.multiply(28 , 28)
    for i in range(n_layers):
        out_features = trial.suggest_int("n_units_l{}".format(i), 4, 128)
        layers.append(nn.Linear(in_features, out_features))
        layers.append(nn.ReLU())
        p = trial.suggest_float("dropout_l{}".format(i), 0.2, 0.5)
        layers.append(nn.Dropout(p))

        in_features = out_features
    layers.append(nn.Linear(in_features, CLASSES))
    layers.append(nn.LogSoftmax(dim=1))

    return nn.Sequential(*layers)

# Loading train and validation data

def get_mnist():
    # Loading FashionMNIST dataset.
    train_loader = torch.utils.data.DataLoader(
        datasets.FashionMNIST(DIR, train=True, download=True, transform=transforms.ToTensor()),
        batch_size=BATCHSIZE,
        shuffle=True,
    )
    valid_loader = torch.utils.data.DataLoader(
        datasets.FashionMNIST(DIR, train=False, transform=transforms.ToTensor()),
        batch_size=BATCHSIZE,
        shuffle=True,
    )

    return train_loader, valid_loader

# defining objective for an optuna trial
def objective(trial):

    # Generating the model.
    model = define_model(trial).to(DEVICE)

    # Generating the optimizers. Variables under "trial" object will be optimized by optuna
    optimizer_name = trial.suggest_categorical("optimizer", ["Adam", "RMSprop", "SGD"])
    lr = trial.suggest_float("lr", 1e-5, 1e-1, log=True)
    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)

    # Get the FashionMNIST dataset.
    train_loader, valid_loader = get_mnist()

    # Training of the model.
    for epoch in range(EPOCHS):
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            # Limiting training data for faster epochs.
            if np.multiply(batch_idx , BATCHSIZE) >= N_TRAIN_EXAMPLES:
                break
            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)
            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            loss.backward()
            optimizer.step()

        # Validation of the model.
        model.eval()
        correct = 0
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(valid_loader):
                # Limiting validation data for faster epochs.
                if np.multiply(batch_idx , BATCHSIZE) >= N_VALID_EXAMPLES:
                    break
                data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)
                output = model(data)
                # Get the index of the max log-probability.
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()

        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)
        trial.report(accuracy, epoch)

        # Handling pruning based on the intermediate value (accuracy).
        # Pruned trials will not complete all epochs resulting in fast completion of all trials
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()
    return accuracy

#The main function
if __name__ == "__main__":
    study = optuna.create_study(direction="maximize")

    #optuna is set to try 100 trials max
    study.optimize(objective, n_trials=100, timeout=600)

    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])
    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])

    print("Study statistics: ")
    print("  Number of finished trials: ", len(study.trials))
    print("  Number of pruned trials: ", len(pruned_trials))
    print("  Number of complete trials: ", len(complete_trials))

    print("Best trial:")
    trial = study.best_trial

    print("  Value: ", trial.value)

    print("  Params: ")
    for key, value in trial.params.items():
        print("    {}: {}".format(key, value))

# checking the best observations for objective value and their trials
optuna.visualization.plot_optimization_history(study)

df = study.trials_dataframe()
df.sort_values(by = 'value', ascending=False).head(5)

#Parallel Coordinate plot for specific hyper-parameters to observe their impact on the objective value (classification accuracy)
optuna.visualization.plot_parallel_coordinate(study, params=["lr", "optimizer", "n_layers"])

#Parallel Coordinate plot for all hyper-parameters to observe their trial range in Optuna
optuna.visualization.plot_parallel_coordinate(study)